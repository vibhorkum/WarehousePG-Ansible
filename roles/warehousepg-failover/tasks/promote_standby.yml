---
# Promote standby coordinator to primary

- name: Get DR coordinator details
  set_fact:
    dr_coordinator_host: "{{ groups['whpg_dr_coordinator'][0] }}"
    dr_coordinator_ip: "{{ hostvars[groups['whpg_dr_coordinator'][0]].private_ip | default(hostvars[groups['whpg_dr_coordinator'][0]].ansible_host) }}"
  run_once: true

- name: Get DR segment hosts
  set_fact:
    dr_segment_hosts: "{{ groups['whpg_dr_segments'] | default([]) }}"
  run_once: true

- name: Check if standby coordinator is running on DR site
  become: yes
  become_user: gpadmin
  shell: |
    source {{ whpg_base_dir }}/greenplum_path.sh
    pg_isready -h localhost -p {{ whpg_coordinator_port }}
  delegate_to: "{{ dr_coordinator_host }}"
  register: standby_running
  changed_when: false
  failed_when: false
  run_once: true

- name: Check if standby data directory exists
  become: yes
  become_user: gpadmin
  stat:
    path: "{{ whpg_coordinator_data_directory }}/postgresql.conf"
  delegate_to: "{{ dr_coordinator_host }}"
  register: standby_data_dir
  run_once: true

- name: Fail if standby data directory does not exist
  assert:
    that:
      - standby_data_dir.stat.exists
    fail_msg: "Standby data directory {{ whpg_coordinator_data_directory }} does not exist on {{ dr_coordinator_host }}."
    success_msg: "Standby data directory exists."
  run_once: true

- name: Check if already promoted (standby.signal indicates standby mode)
  become: yes
  become_user: gpadmin
  stat:
    path: "{{ whpg_coordinator_data_directory }}/standby.signal"
  delegate_to: "{{ dr_coordinator_host }}"
  register: standby_signal
  run_once: true

- name: Set standby promotion status
  set_fact:
    needs_promotion: "{{ standby_signal.stat.exists | default(false) }}"
  run_once: true

- name: Display promotion plan
  debug:
    msg: |
      Standby running: {{ 'Yes' if standby_running.rc == 0 else 'No (stopped by gpstop)' }}
      Needs promotion: {{ 'Yes' if needs_promotion else 'Already promoted or will be activated' }}
      DR takeover mode: {{ whpg_failover_dr_takeover | default(true) }}
  run_once: true

# CRITICAL: For DR takeover, we must prepare DR segment mirrors BEFORE starting the cluster
# Remove standby.signal from DR mirrors so they can start as primaries
- name: Prepare DR segment mirrors for promotion (remove standby.signal)
  become: yes
  become_user: gpadmin
  shell: |
    echo "=== Preparing DR segment mirrors for promotion ==="
    # Use configured directories or fall back to glob pattern
    DIRS="{{ whpg_mirror_data_directories | default([]) | join(' ') }}"
    if [ -z "$DIRS" ]; then
      DIRS=$(ls -d /data/mirror/seg*/ /data/primary/seg*/ /data1/mirror/ /data2/mirror/ 2>/dev/null || true)
    fi
    
    for dir in $DIRS; do
      if [ -d "$dir" ]; then
        echo "Processing $dir"
        # Remove standby.signal to allow starting as primary
        if [ -f "${dir}/standby.signal" ]; then
          echo "  Removing standby.signal"
          rm -f "${dir}/standby.signal"
        fi
        # Also check without trailing slash
        if [ -f "${dir}standby.signal" ]; then
          echo "  Removing standby.signal"
          rm -f "${dir}standby.signal"
        fi
        # Clean up stale postmaster.pid if process not running
        for pidfile in "${dir}/postmaster.pid" "${dir}postmaster.pid"; do
          if [ -f "$pidfile" ]; then
            PID=$(head -1 "$pidfile" 2>/dev/null || echo "")
            if [ -n "$PID" ] && ! kill -0 "$PID" 2>/dev/null; then
              echo "  Removing stale postmaster.pid"
              rm -f "$pidfile"
            fi
          fi
        done
      fi
    done
    # Clear stale shared memory
    echo "Clearing stale shared memory..."
    ipcrm --all=shm 2>/dev/null || for shmid in $(ipcs -m 2>/dev/null | awk '/gpadmin/ {print $2}'); do
      ipcrm -m "$shmid" 2>/dev/null || true
    done
    echo "DR segment preparation complete"
  delegate_to: "{{ item }}"
  loop: "{{ dr_segment_hosts }}"
  when:
    - whpg_failover_dr_takeover | default(true)
    - dr_segment_hosts | length > 0
  ignore_errors: yes

# gpactivatestandby handles both starting the standby if stopped AND promoting it
# The -f flag forces activation even if standby is not running
- name: Activate and promote standby coordinator using gpactivatestandby
  become: yes
  become_user: gpadmin
  shell: |
    source {{ whpg_base_dir }}/greenplum_path.sh
    export COORDINATOR_DATA_DIRECTORY={{ whpg_coordinator_data_directory }}
    export PGPORT={{ whpg_coordinator_port }}
    # Force activation - this will work even if standby was stopped by gpstop
    gpactivatestandby -d {{ whpg_coordinator_data_directory }} -a -f
  delegate_to: "{{ dr_coordinator_host }}"
  register: promote_standby
  failed_when: false
  run_once: true

- name: Display promotion result
  debug:
    msg: "{{ promote_standby.stdout_lines }}"
  run_once: true

# gpactivatestandby starts in single-node mode, we need to restart the full cluster
- name: Stop coordinator to restart in full mode
  become: yes
  become_user: gpadmin
  shell: |
    source {{ whpg_base_dir }}/greenplum_path.sh
    export COORDINATOR_DATA_DIRECTORY={{ whpg_coordinator_data_directory }}
    gpstop -a -M fast || true
  delegate_to: "{{ dr_coordinator_host }}"
  register: stop_result
  run_once: true

# CRITICAL: Before starting the cluster, we need to ensure DR segments can start
# The standby.signal removal should have already happened, but double-check
- name: Final preparation of DR segments before cluster start
  become: yes
  become_user: gpadmin
  shell: |
    echo "=== Final DR segment preparation ==="
    # Use configured directories or fall back to glob pattern
    DIRS="{{ whpg_mirror_data_directories | default([]) | join(' ') }}"
    if [ -z "$DIRS" ]; then
      DIRS=$(ls -d /data/mirror/seg*/ /data/primary/seg*/ /data1/mirror/ /data2/mirror/ 2>/dev/null || true)
    fi
    
    for dir in $DIRS; do
      if [ -d "$dir" ]; then
        # Remove standby.signal
        rm -f "${dir}/standby.signal" "${dir}standby.signal" 2>/dev/null || true
        # Remove stale postmaster.pid
        for pidfile in "${dir}/postmaster.pid" "${dir}postmaster.pid"; do
          if [ -f "$pidfile" ]; then
            PID=$(head -1 "$pidfile" 2>/dev/null || echo "")
            if [ -z "$PID" ] || ! kill -0 "$PID" 2>/dev/null; then
              rm -f "$pidfile"
            fi
          fi
        done
      fi
    done
    # Clear shared memory
    ipcrm --all=shm 2>/dev/null || true
    echo "DR segments ready"
  delegate_to: "{{ item }}"
  loop: "{{ dr_segment_hosts }}"
  when:
    - whpg_failover_dr_takeover | default(true)
    - dr_segment_hosts | length > 0
  ignore_errors: yes

- name: Start full cluster from new primary
  become: yes
  become_user: gpadmin
  shell: |
    source {{ whpg_base_dir }}/greenplum_path.sh
    export COORDINATOR_DATA_DIRECTORY={{ whpg_coordinator_data_directory }}
    
    echo "=== Starting cluster after failover ==="
    
    # First, try starting normally
    # The cluster may take time due to DTX recovery if there were in-flight transactions
    timeout 300 gpstart -a 2>&1 || {
      EXIT_CODE=$?
      echo "gpstart returned: $EXIT_CODE"
      
      # If gpstart failed/timed out, the coordinator might still be running
      # Check if coordinator is accepting connections
      if pg_isready -h localhost -p {{ whpg_coordinator_port }} -t 5; then
        echo "Coordinator is running and accepting connections"
      else
        echo "Coordinator not ready, attempting recovery start..."
        # Try starting with skip of standby
        gpstart -a -y 2>&1 || true
      fi
    }
    
    # Final check
    sleep 5
    pg_isready -h localhost -p {{ whpg_coordinator_port }} -t 10 && echo "Cluster started successfully" || echo "Cluster may still be starting"
  delegate_to: "{{ dr_coordinator_host }}"
  register: start_result
  failed_when: false
  run_once: true

- name: Display cluster start result
  debug:
    msg: "{{ start_result.stdout_lines }}"
  run_once: true

# If cluster start timed out due to DTX recovery, wait and check again
- name: Wait for DTX recovery to complete (may take a few minutes)
  become: yes
  become_user: gpadmin
  shell: |
    source {{ whpg_base_dir }}/greenplum_path.sh
    export COORDINATOR_DATA_DIRECTORY={{ whpg_coordinator_data_directory }}
    
    echo "Checking cluster readiness..."
    for i in $(seq 1 30); do
      if psql -d template1 -t -c "SELECT 1;" 2>/dev/null | grep -q 1; then
        echo "Cluster is ready (attempt $i)"
        exit 0
      fi
      echo "Waiting for cluster... (attempt $i/30)"
      sleep 10
    done
    echo "Cluster may still be in recovery"
    exit 1
  delegate_to: "{{ dr_coordinator_host }}"
  register: cluster_ready
  failed_when: false
  run_once: true

- name: Display cluster readiness
  debug:
    msg: "{{ cluster_ready.stdout_lines }}"
  run_once: true

- name: Wait for promoted coordinator to be ready
  become: yes
  become_user: gpadmin
  shell: |
    source {{ whpg_base_dir }}/greenplum_path.sh
    pg_isready -h localhost -p {{ whpg_coordinator_port }} -t {{ whpg_startup_timeout }}
  delegate_to: "{{ dr_coordinator_host }}"
  register: new_primary_ready
  retries: 6
  delay: 10
  until: new_primary_ready.rc == 0
  run_once: true
  failed_when: false

- name: Verify new primary coordinator is accepting connections
  become: yes
  become_user: gpadmin
  shell: |
    source {{ whpg_base_dir }}/greenplum_path.sh
    export COORDINATOR_DATA_DIRECTORY={{ whpg_coordinator_data_directory }}
    psql -d template1 -c "SELECT 1;"
  delegate_to: "{{ dr_coordinator_host }}"
  register: new_primary_test
  changed_when: false
  failed_when: false
  run_once: true

- name: Display new primary status
  debug:
    msg:
      - "Standby coordinator has been promoted to primary"
      - "New primary coordinator: {{ dr_coordinator_host }}"
      - "Connection test: {{ 'SUCCESS' if new_primary_test.rc == 0 else 'FAILED - DTX recovery may be in progress' }}"
  run_once: true
